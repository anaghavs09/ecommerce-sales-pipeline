{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5266667e",
   "metadata": {},
   "source": [
    "# üõí E-Commerce Sales Data Analysis Pipeline\n",
    "## Data Exploration & Quality Assessment\n",
    "\n",
    "---\n",
    "\n",
    "### üìã Project Overview\n",
    "This notebook explores raw e-commerce data from the Brazilian marketplace Olist (2016-2018). \n",
    "We analyze **100K+ orders** across multiple datasets to identify data quality issues before \n",
    "building our ETL pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Objectives\n",
    "1. **Load** real-world e-commerce data from CSV files\n",
    "2. **Explore** data structure, types, and relationships\n",
    "3. **Identify** data quality issues (missing values, wrong types, duplicates)\n",
    "4. **Document** findings for the data cleaning phase\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Datasets Used\n",
    "\n",
    "| Dataset | Records | Description |\n",
    "|---------|---------|-------------|\n",
    "| **Customers** | ~99K | Customer information (ID, location) |\n",
    "| **Orders** | ~99K | Order details (status, timestamps) |\n",
    "| **Order Items** | ~112K | Products in each order |\n",
    "| **Products** | ~32K | Product catalog (names, categories, prices) |\n",
    "| **Sellers** | ~3K | Seller information |\n",
    "| **Payments** | ~103K | Payment transactions |\n",
    "| **Reviews** | ~100K | Customer ratings and comments |\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Tools & Libraries\n",
    "- **pandas**: Data manipulation and analysis\n",
    "- **Python**: Data exploration and scripting\n",
    "- **PostgreSQL**: Target database \n",
    "- **Tableau**: Visualization\n",
    "\n",
    "---\n",
    "\n",
    "### üìÇ Data Source\n",
    "**Kaggle:** [Brazilian E-Commerce Public Dataset by Olist](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)\n",
    "\n",
    "**Files located in:** `../data/raw/`\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ What We'll Discover\n",
    "\n",
    "#### Expected Data Quality Issues:\n",
    "- ‚ùå Date columns stored as text (need datetime conversion)\n",
    "- ‚ùå Missing values in various fields\n",
    "- ‚ùå Potential duplicates\n",
    "- ‚ùå Product names in Portuguese (may need translation)\n",
    "- ‚ùå Inconsistent formatting\n",
    "\n",
    "#### Key Questions:\n",
    "- How much data is missing?\n",
    "- Are dates in the correct format?\n",
    "- Do we have duplicates?\n",
    "- What's the date range of our data?\n",
    "- How many unique customers vs orders?\n",
    "\n",
    "---\n",
    "\n",
    "### üìù Next Steps (After This Notebook)\n",
    "1. **Data Cleaning** (`clean_data.py`) - Fix identified issues with pandas\n",
    "2. **Database Design** - Create star schema for analytics\n",
    "3. **ETL Pipeline** - Load cleaned data to PostgreSQL\n",
    "4. **SQL Analysis** - Business metrics and customer insights\n",
    "5. **Visualization** - Tableau dashboard\n",
    "\n",
    "---\n",
    "\n",
    "**Let's begin the exploration! üëá**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ed90acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fc03ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to show all columns when displaying data\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# displays up to 100 rows instead of default 10\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49066709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available files: \n",
      " olist_sellers_dataset.csv:  0.17 MB\n",
      " product_category_name_translation.csv:  0.00 MB\n",
      " olist_orders_dataset.csv:  16.84 MB\n",
      " olist_order_items_dataset.csv:  14.72 MB\n",
      " olist_customers_dataset.csv:  8.62 MB\n",
      " olist_geolocation_dataset.csv:  58.44 MB\n",
      " olist_order_payments_dataset.csv:  5.51 MB\n",
      " olist_order_reviews_dataset.csv:  13.78 MB\n",
      " olist_products_dataset.csv:  2.27 MB\n"
     ]
    }
   ],
   "source": [
    "# File path\n",
    "data_dir = '../data/raw/'\n",
    "\n",
    "print(\"Available files: \")\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith('.csv'):\n",
    "        size = os.path.getsize(os.path.join(data_dir, file)) / 1024 / 1024 # bytes->kilobytes->megabytes\n",
    "        print(f\" {file}: {size: .2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15c10354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "‚úÖ Loaded!\n",
      "Customers: 99,441 rows\n",
      "Orders: 99,441 rows\n",
      "Order Items: 112,650 rows\n",
      "Products: 32,951 rows\n"
     ]
    }
   ],
   "source": [
    "# Load main datasets\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "customers = pd.read_csv(data_dir + 'olist_customers_dataset.csv')\n",
    "orders = pd.read_csv(data_dir + 'olist_orders_dataset.csv')\n",
    "order_items = pd.read_csv(data_dir + 'olist_order_items_dataset.csv')\n",
    "products = pd.read_csv(data_dir + 'olist_products_dataset.csv')\n",
    "\n",
    "print(\"‚úÖ Loaded!\")\n",
    "print(f\"Customers: {len(customers):,} rows\") #:, - formats the number with commas like 1,000\n",
    "print(f\"Orders: {len(orders):,} rows\")\n",
    "print(f\"Order Items: {len(order_items):,} rows\")\n",
    "print(f\"Products: {len(products):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4ea7cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " CUSTOMERS DATA\n",
      "==================================================\n",
      "\n",
      "First 5 rows: \n",
      "                        customer_id                customer_unique_id  \\\n",
      "0  06b8999e2fba1a1fbc88172c00ba8bc7  861eff4711a542e4b93843c6dd7febb0   \n",
      "1  18955e83d337fd6b2def6b18a428ac77  290c77bc529b7ac935b93aa66c333dc3   \n",
      "2  4e7b3e00288586ebd08712fdd0374a03  060e732b5b29e8181a18229c7b0b2b5e   \n",
      "3  b2b6027bc5c5109e529d4dc6358b12c3  259dac757896d24d7702b9acbbff3f3c   \n",
      "4  4f2d8ab171c80ec8364f7c12e35b23ad  345ecd01c38d18a9036ed96c73b8d066   \n",
      "\n",
      "   customer_zip_code_prefix          customer_city customer_state  \n",
      "0                     14409                 franca             SP  \n",
      "1                      9790  sao bernardo do campo             SP  \n",
      "2                      1151              sao paulo             SP  \n",
      "3                      8775        mogi das cruzes             SP  \n",
      "4                     13056               campinas             SP  \n",
      "\n",
      "Column names: \n",
      "['customer_id', 'customer_unique_id', 'customer_zip_code_prefix', 'customer_city', 'customer_state']\n",
      "\n",
      "Data Types: \n",
      "customer_id                 object\n",
      "customer_unique_id          object\n",
      "customer_zip_code_prefix     int64\n",
      "customer_city               object\n",
      "customer_state              object\n",
      "dtype: object\n",
      "\n",
      "DataFrame shape (rows, columns): \n",
      "(99441, 5)\n",
      "\n",
      "Missing values: \n",
      "customer_id                 0\n",
      "customer_unique_id          0\n",
      "customer_zip_code_prefix    0\n",
      "customer_city               0\n",
      "customer_state              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n CUSTOMERS DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nFirst 5 rows: \")\n",
    "print(customers.head())\n",
    "\n",
    "print(\"\\nColumn names: \")\n",
    "print(customers.columns.tolist())\n",
    "\n",
    "print(\"\\nData Types: \")\n",
    "print(customers.dtypes)\n",
    "\n",
    "print(\"\\nDataFrame shape (rows, columns): \")\n",
    "print(customers.shape)\n",
    "\n",
    "print(\"\\nMissing values: \")\n",
    "print(customers.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c883bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ORDERS DATA\n",
      "==================================================\n",
      "\n",
      "First 5 rows: \n",
      "                           order_id                       customer_id  \\\n",
      "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
      "1  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
      "2  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
      "3  949d5b44dbf5de918fe9c16f97b45f8a  f88197465ea7920adcdbec7375364d82   \n",
      "4  ad21c59c0840e6cb83a9ceb5573f8159  8ab97904e6daea8866dbdbc4fb7aad2c   \n",
      "\n",
      "  order_status order_purchase_timestamp    order_approved_at  \\\n",
      "0    delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
      "1    delivered      2018-07-24 20:41:37  2018-07-26 03:24:27   \n",
      "2    delivered      2018-08-08 08:38:49  2018-08-08 08:55:23   \n",
      "3    delivered      2017-11-18 19:28:06  2017-11-18 19:45:59   \n",
      "4    delivered      2018-02-13 21:18:39  2018-02-13 22:20:29   \n",
      "\n",
      "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
      "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
      "1          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
      "2          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
      "3          2017-11-22 13:39:59           2017-12-02 00:28:42   \n",
      "4          2018-02-14 19:46:34           2018-02-16 18:17:02   \n",
      "\n",
      "  order_estimated_delivery_date  \n",
      "0           2017-10-18 00:00:00  \n",
      "1           2018-08-13 00:00:00  \n",
      "2           2018-09-04 00:00:00  \n",
      "3           2017-12-15 00:00:00  \n",
      "4           2018-02-26 00:00:00  \n",
      "\n",
      "Columns: \n",
      "['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date']\n",
      "\n",
      "Data types: \n",
      "order_id                         object\n",
      "customer_id                      object\n",
      "order_status                     object\n",
      "order_purchase_timestamp         object\n",
      "order_approved_at                object\n",
      "order_delivered_carrier_date     object\n",
      "order_delivered_customer_date    object\n",
      "order_estimated_delivery_date    object\n",
      "dtype: object\n",
      "\n",
      "Missing values: \n",
      "order_approved_at                 160\n",
      "order_delivered_carrier_date     1783\n",
      "order_delivered_customer_date    2965\n",
      "dtype: int64\n",
      "\n",
      "Order statuses: \n",
      "order_status\n",
      "delivered      96478\n",
      "shipped         1107\n",
      "canceled         625\n",
      "unavailable      609\n",
      "invoiced         314\n",
      "processing       301\n",
      "created            5\n",
      "approved           2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Date range: \n",
      "From: 2016-09-04 21:15:19\n",
      "To: 2018-10-17 17:30:18\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nORDERS DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nFirst 5 rows: \")\n",
    "print(orders.head())\n",
    "\n",
    "print(\"\\nColumns: \")\n",
    "print(orders.columns.tolist())\n",
    "\n",
    "print(\"\\nData types: \")\n",
    "print(orders.dtypes)\n",
    "\n",
    "print(\"\\nMissing values: \")\n",
    "missing = orders.isnull().sum()\n",
    "print(missing[missing > 0]) # show columns with nulls only\n",
    "\n",
    "print(\"\\nOrder statuses: \")\n",
    "print(orders['order_status'].value_counts())\n",
    "\n",
    "print(\"\\nDate range: \")\n",
    "print(f\"From: {orders['order_purchase_timestamp'].min()}\")\n",
    "print(f\"To: {orders['order_purchase_timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cef16241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DUPLICATES CHECK\n",
      "==================================================\n",
      "Duplicate customers: 0\n",
      "Duplicated customer IDs: 0\n",
      "\n",
      "Duplicate orders: 0\n",
      "Duplicate order IDs: 0\n",
      "\n",
      "Duplicate order items: 0\n",
      "\n",
      "Customers with multiple orders:\n",
      "  Total unique customers: 99441\n",
      "  Customers with 2+ orders: 0\n",
      "  Max orders by one customer: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDUPLICATES CHECK\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Duplicate customers: {customers.duplicated().sum()}\")\n",
    "print(f\"Duplicated customer IDs: {customers['customer_id'].duplicated().sum()}\")\n",
    "\n",
    "print(f\"\\nDuplicate orders: {orders.duplicated().sum()}\")\n",
    "print(f\"Duplicate order IDs: {orders['order_id'].duplicated().sum()}\")\n",
    "\n",
    "print(f\"\\nDuplicate order items: {order_items.duplicated().sum()}\")\n",
    "\n",
    "# Check if same customer ordered multiple times (expected)\n",
    "print(f\"\\nCustomers with multiple orders:\")\n",
    "customer_order_counts = orders['customer_id'].value_counts()\n",
    "print(f\"  Total unique customers: {len(customer_order_counts)}\")\n",
    "print(f\"  Customers with 2+ orders: {(customer_order_counts > 1).sum()}\")\n",
    "print(f\"  Max orders by one customer: {customer_order_counts.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d6ef6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DATA QUALITY ISSUES TO FIX: \n",
      "==================================================\n",
      "1. Date columns stored as text(need conversion)\n",
      "2. Missing values in orders: 4908 cells\n",
      "3. Missing values in products: 2448 cells\n",
      "\n",
      "‚úÖ Total issues to address: 3\n",
      "\n",
      "üìù Next step: Clean these issues with pandas!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDATA QUALITY ISSUES TO FIX: \")\n",
    "print(\"=\"*50)\n",
    "\n",
    "issues = []\n",
    "\n",
    "# Check date columns\n",
    "date_cols = [col for col in orders.columns if 'timestamp' in col or 'date' in col]\n",
    "if any(orders[col].dtype == 'object' for col in date_cols):\n",
    "    issues.append(\"Date columns stored as text(need conversion)\")\n",
    "\n",
    "# Check orders for nulls\n",
    "if orders.isnull().sum().sum() > 0:\n",
    "    issues.append(f\"Missing values in orders: {orders.isnull().sum().sum()} cells\")\n",
    "\n",
    "# Check products for nulls\n",
    "if products.isnull().sum().sum() > 0:\n",
    "    issues.append(f\"Missing values in products: {products.isnull().sum().sum()} cells\")\n",
    "\n",
    "# Display issues\n",
    "for i, issue in enumerate(issues, 1):\n",
    "    print(f\"{i}. {issue}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total issues to address: {len(issues)}\")\n",
    "print(\"\\nüìù Next step: Clean these issues with pandas!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
